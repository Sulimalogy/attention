{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc64c58-fcc2-4b08-bb57-6437bd047ce8",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "## what is a transformer \n",
    "\n",
    "GPT \n",
    "generative \n",
    "make output \n",
    "transformer \n",
    "pretrin is fine tuning \n",
    "\n",
    "the Transformer architecture, including attention mechanisms, positional encodings, and matrix operations:\n",
    "how data flow inside a transformer \n",
    "\n",
    "the inputs will be deivded into into words  tokens if  the input is images or sound it will be deviced into chunks of pixles or audio snipit \n",
    "\n",
    "each word will be converted into a vector that coads the meaning of the word \n",
    "\n",
    "think of a vector in 3d words with simmiller meanings are cloe to each others\n",
    "\n",
    "this sequance of vectors will pass into attention block\n",
    "## follow the data \n",
    " audio and image \n",
    " takes text  and output a prediction of the probability destribution \n",
    "\n",
    "embedding > attention > Feed forward > un embeding  \n",
    "\n",
    " # embeding block \n",
    "\n",
    " # attention block \n",
    "\n",
    " # feed forward bock \n",
    " \n",
    "## Key Components:\n",
    "\n",
    "   * Scaled Dot-Product Attention: Core of the attention mechanism.\n",
    "   * Multi-Head Attention: Allows the model to attend to different parts of the input.\n",
    "   * Feed-Forward Networks: Standard neural layers in the transformer.\n",
    "   * Positional Encoding: To give tokens information about their positions.\n",
    "   * Layer Normalization: Used to stabilize training.\n",
    "     \n",
    "   * Feed-Forward Networks: A simple linear transformation after attention.\n",
    "   * Positional Encoding: Adding position information to tokens.\n",
    "   * Transformer Architecture: Stacking encoder/decoder layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd1fbd-f6c3-4bf9-9a45-da08e6b95938",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "\n",
    "    d_k = Q.shape[-1]\n",
    "\n",
    "    # Calculate the attention scores (QK^T / sqrt(d_k))\n",
    "    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = np.where(mask == 0, -1e9, scores)  # Apply mask\n",
    "\n",
    "    # Softmax to get the attention weights\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "\n",
    "    # Weight the values\n",
    "    output = np.matmul(attention_weights, V)\n",
    "\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f21299b-eb04-4c72-bdfe-4e1c9a2bf1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "\n",
    "    Args:\n",
    "        Q: Queries matrix (batch_size, seq_len_q, d_k)\n",
    "        K: Keys matrix (batch_size, seq_len_k, d_k)\n",
    "        V: Values matrix (batch_size, seq_len_k, d_v)\n",
    "        mask: Optional mask to exclude some positions\n",
    "\n",
    "    Returns:\n",
    "        Attention-weighted output\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a8f1c-7c39-41cd-8cc5-f833b1b0a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Initialize the multi-head attention layer.\n",
    "\n",
    "        Args:\n",
    "            d_model: Dimension of the model\n",
    "            num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Random weight matrices for linear transformations\n",
    "        self.W_q = np.random.randn(d_model, d_model)\n",
    "        self.W_k = np.random.randn(d_model, d_model)\n",
    "        self.W_v = np.random.randn(d_model, d_model)\n",
    "        self.W_o = np.random.randn(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return x.reshape(batch_size, -1, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Apply multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            Q, K, V: Query, Key, Value matrices\n",
    "            mask: Optional mask\n",
    "\n",
    "        Returns:\n",
    "            Output of multi-head attention\n",
    "        \"\"\"\n",
    "        # Linear transformations\n",
    "        Q = np.dot(Q, self.W_q)\n",
    "        K = np.dot(K, self.W_k)\n",
    "        V = np.dot(V, self.W_v)\n",
    "\n",
    "        # Split the heads\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "\n",
    "        # Apply scaled dot-product attention\n",
    "        attention = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine the heads back\n",
    "        attention = attention.transpose(0, 2, 1, 3).reshape(Q.shape[0], -1, self.d_model)\n",
    "\n",
    "        # Final linear transformation\n",
    "        output = np.dot(attention, self.W_o)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "np.random.seed(42)\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "# Random queries, keys, values\n",
    "Q = np.random.randn(batch_size, seq_len, d_model)\n",
    "K = np.random.randn(batch_size, seq_len, d_model)\n",
    "V = np.random.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Instantiate and run Multi-Head Attention\n",
    "multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "output = multi_head_attention.forward(Q, K, V)\n",
    "print(\"Output shape:\", output.shape)  # Should be (batch_size, seq_len, d_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FeedForwardNetwork:\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        \"\"\"\n",
    "        Initialize the feed-forward network.\n",
    "\n",
    "        Args:\n",
    "            d_model: Dimension of the model\n",
    "            d_ff: Dimension of the feed-forward network\n",
    "        \"\"\"\n",
    "        self.W1 = np.random.randn(d_model, d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model)\n",
    "        self.b1 = np.zeros((1, d_ff))\n",
    "        self.b2 = np.zeros((1, d_model))\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply the feed-forward network.\n",
    "\n",
    "        Args:\n",
    "            x: Input matrix (batch_size, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "            Output of the feed-forward network\n",
    "        \"\"\"\n",
    "        z1 = np.dot(x, self.W1) + self.b1\n",
    "        a1 = self.relu(z1)\n",
    "        z2 = np.dot(a1, self.W2) + self.b2\n",
    "        return z2\n",
    "\n",
    "# Example usage:\n",
    "# Initialize FFN with d_model=512 and d_ff=2048\n",
    "ffn = FeedForwardNetwork(512, 2048)\n",
    "output_ffn = ffn.forward(np.random.randn(2, 10, 512))  # Example input\n",
    "print(\"Feed-Forward output shape:\", output_ffn.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c190ec5-fe3e-4b1b-b1b3-113abe7f7706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding:\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Initialize positional encoding.\n",
    "\n",
    "        Args:\n",
    "            d_model: Dimension of the model\n",
    "            max_len: Maximum length of the input sequence\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.pos_encoding = self.create_positional_encoding()\n",
    "\n",
    "    def create_positional_encoding(self):\n",
    "        \"\"\"\n",
    "        Create positional encoding for the input sequence.\n",
    "\n",
    "        Returns:\n",
    "            Positional encoding matrix (max_len, d_model)\n",
    "        \"\"\"\n",
    "        position = np.arange(self.max_len).reshape(-1, 1)\n",
    "        div_term = np.exp(np.arange(0, self.d_model, 2) * -(np.log(10000.0) / self.d_model))\n",
    "\n",
    "        pos_encoding = np.zeros((self.max_len, self.d_model))\n",
    "        pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = np.cos(position * div_term)\n",
    "\n",
    "        return pos_encoding\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply positional encoding to input.\n",
    "\n",
    "        Args:\n",
    "            x: Input matrix (batch_size, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "            Input with positional encoding applied\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        return x + self.pos_encoding[:seq_len, :]\n",
    "\n",
    "# Example usage:\n",
    "# Initialize positional encoding with d_model=512\n",
    "pos_enc = PositionalEncoding(512)\n",
    "x_input = np.random.randn(2, 10, 512)  # Example input\n",
    "output_pos_enc = pos_enc.forward(x_input)\n",
    "print(\"Positional encoding output shape:\", output_pos_enc.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4f5e2b-7d8c-460e-b5c2-5333f7255305",
   "metadata": {},
   "source": [
    "\n",
    "\"\"\" # Feed-Forward example integration:\n",
    "ffn = FeedForwardNetwork(512, 2048)\n",
    "output_ffn = ffn.forward(attention_output)  # Use after the attention mechanism\n",
    "\n",
    "# Positional Encoding example integration:\n",
    "pos_enc = PositionalEncoding(512)\n",
    "x_with_pos = pos_enc.forward(input_embeddings)  # Add positional encoding to the embeddings\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
